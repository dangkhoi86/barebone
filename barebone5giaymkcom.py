import requests
from bs4 import BeautifulSoup
import re
import time
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
import json
import urllib.parse
from gspread_formatting import format_cell_range, CellFormat, TextFormat, set_column_width, Color, Padding
from tenacity import retry, stop_after_attempt, wait_exponential
import os
SHEET_URL = os.environ.get("SHEET_URL")

SHEET_5GIAY_URL = SHEET_URL  # D√πng c√πng 1 link Google Sheet
today_str = datetime.now().strftime("%d-%m-%Y")
compare_sheet_name = f"Check-Gia-MKCOM-{today_str}"

def get_all_barebone_links():
    base_url = "https://minhkhoicomputer.com/product-category/danh-muc-san-pham/barabone/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    product_links = dict()
    page = 1
    while True:
        url = base_url if page == 1 else f"{base_url}page/{page}/"
        print(f"ƒêang l·∫•y link t·ª´: {url}")
        resp = requests.get(url, headers=headers)
        if resp.status_code != 200:
            print("  -> L·ªñI: Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c trang danh m·ª•c n√†y!")
            break
        soup = BeautifulSoup(resp.text, "html.parser")
        for a in soup.find_all('a', href=True):
            if '/product/' in a['href']:
                full_link = a['href'] if a['href'].startswith('http') else f"https://minhkhoicomputer.com{a['href']}"
                if full_link not in product_links:
                    product_links[full_link] = page
        next_btn = soup.find('a', class_='next page-numbers')
        if not next_btn:
            break
        page += 1
        time.sleep(1)
    # Tr·∫£ v·ªÅ list c√°c tuple (link, page)
    return [(link, product_links[link]) for link in product_links]

def get_barebone_info(url, page):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    resp = requests.get(url, headers=headers)
    if resp.status_code != 200:
        print(f"L·ªñI: Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c s·∫£n ph·∫©m {url}")
        return []
    soup = BeautifulSoup(resp.text, "html.parser")
    # L·∫•y b·∫£ng th√¥ng tin
    table = soup.find('table', class_=['notcauhinh', 'cauhinh'])
    results = []
    if table:
        rows = table.find_all('tr')
        for row in rows[1:]:  # b·ªè header
            tds = row.find_all('td')
            if len(tds) > 1:
                name = tds[0].text.strip()
                # X√≥a th√¥ng tin trong d·∫•u ngo·∫∑c ƒë∆°n
                name = re.sub(r'\([^)]*\)', '', name).strip()
                # Th√™m "[ƒêang ·∫®n]" n·∫øu tr c√≥ thu·ªôc t√≠nh hidden
                if row.has_attr('hidden'):
                    name = f"{name} [ƒêang ·∫®n]"
                # Ch·ªâ l·∫•y d√≤ng c√≥ ch·ªØ 'barebone' (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
                if 'barebone' not in name.lower():
                    continue
                price_text = tds[1].text
                strong = tds[1].find('strong')
                if strong:
                    price_text = strong.text
                price = price_text.strip().replace('.', '').replace(',', '').replace('VND', '').replace('ƒë', '')
                try:
                    price = int(price)
                except:
                    price = None
                print(f"T√™n: {name} | Gi√°: {price} | Link: {url} | Trang: {page}")
                post_title = ""
                h2 = soup.find('h2', class_='product-name')
                if h2:
                    post_title = h2.text.strip()
                results.append({
                    "T√™n Post": post_title,
                    "T√™n s·∫£n ph·∫©m": name,
                    "Gi√° b√°n (VNƒê)": price,
                    "Link": url,
                    "Trang": page
                })
    else:
        print(f"Kh√¥ng t√¨m th·∫•y b·∫£ng th√¥ng tin t·∫°i {url}")
    return results

def upload_to_gsheets(df, sheet_url, worksheet_name="Sheet1"):
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    sh = client.open_by_url(sheet_url)
    try:
        worksheet = sh.worksheet(worksheet_name)
        worksheet.clear()
    except:
        worksheet = sh.add_worksheet(title=worksheet_name, rows="300", cols="5")
    # Thay th·∫ø NaN b·∫±ng None ho·∫∑c chu·ªói r·ªóng
    df = df.fillna("")  # Ho·∫∑c df = df.fillna(None)
    # Ghi header + data
    worksheet.update([df.columns.values.tolist()] + df.values.tolist())
    print(f"ƒê√£ upload d·ªØ li·ªáu l√™n Google Sheets: {worksheet_name}")

    # Sau khi ƒë√£ upload d·ªØ li·ªáu l√™n sheet v√† c√≥ bi·∫øn worksheet
    header_format = CellFormat(
        backgroundColor=Color(0.2, 0.6, 0.86),  # Xanh d∆∞∆°ng nh·∫°t, ƒë·ªïi theo √Ω b·∫°n
        textFormat=TextFormat(bold=True, fontFamily='Arial', fontSize=11, foregroundColor=Color(1,1,1)),  # Ch·ªØ tr·∫Øng, in ƒë·∫≠m
        padding=Padding(left=5, top=5, right=5, bottom=5)
    )
    format_cell_range(worksheet, 'A1:G1', header_format)

    number_format = CellFormat(
        numberFormat={'type': 'NUMBER', 'pattern': '#,##0'},
        horizontalAlignment='RIGHT'
    )

    format_cell_range(worksheet, 'C1:E1000', number_format)

    padding_format = CellFormat(
        padding=Padding(left=5, top=5, right=5, bottom=5)
    )
    format_cell_range(worksheet, 'A2:F1000', padding_format)

    set_column_width(worksheet, 'A', 300)
    set_column_width(worksheet, 'B', 400)
    set_column_width(worksheet, 'C', 111)
    set_column_width(worksheet, 'D', 111)
    set_column_width(worksheet, 'E', 91)
    set_column_width(worksheet, 'F', 200)
    set_column_width(worksheet, 'G', 200)

    left_middle_format = CellFormat(
        horizontalAlignment='LEFT',
        verticalAlignment='MIDDLE'
    )
    format_cell_range(worksheet, 'A1:A1000', left_middle_format)

    left_format = CellFormat(
        horizontalAlignment='LEFT',
    )
    format_cell_range(worksheet, 'B1:B1000', left_format)
    format_cell_range(worksheet, 'F1:G1000', left_format)

    middle_format = CellFormat(
        verticalAlignment='MIDDLE'
    )
    format_cell_range(worksheet, 'A1:G1000', middle_format)

    worksheet.freeze(rows=1)
    # worksheet.freeze(rows=1, cols=1)

    return worksheet

def merge_link_cells(sheet_url, worksheet_name="Sheet1", link_col=3):
    # link_col: c·ªôt Link, m·∫∑c ƒë·ªãnh l√† c·ªôt th·ª© 3 (A=1, B=2, C=3, ...)
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    sh = client.open_by_url(sheet_url)
    worksheet = sh.worksheet(worksheet_name)
    data = worksheet.get_all_values()
    last_link = None
    start_row = None
    for i, row in enumerate(data[1:], start=2):  # B·ªè header, b·∫Øt ƒë·∫ßu t·ª´ d√≤ng 2
        link = row[link_col-1]
        if link == last_link:
            # ƒêang trong chu·ªói gi·ªëng nhau
            continue
        else:
            # N·∫øu c√≥ chu·ªói tr∆∞·ªõc ƒë√≥ d√†i h∆°n 1, th√¨ merge
            if start_row and i-1 > start_row:
                worksheet.merge_cells(
                    start_row, link_col, i-1, link_col
                )
            last_link = link
            start_row = i
    # Merge chu·ªói cu·ªëi c√πng n·∫øu c·∫ßn
    if start_row and len(data) > start_row:
        worksheet.merge_cells(
            start_row, link_col, len(data), link_col
        )
    print("ƒê√£ merge c√°c √¥ link gi·ªëng nhau.")

def has_factor(part):
    # Ki·ªÉm tra part c√≥ ch·ª©a 1 trong c√°c factor kh√¥ng
    return bool(re.search(r'\b(sff|mt|dt|mini|tiny)\b', part, re.IGNORECASE))

def get_all_5giay_prices(sheet_url, sheet_date):
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    sh = client.open_by_url(sheet_url)
    worksheet = sh.worksheet(sheet_date)
    data = worksheet.get_all_records()
    
    # In ra t√™n c√°c c·ªôt ƒë·ªÉ ki·ªÉm tra
    print("C√°c c·ªôt trong sheet:", list(data[0].keys()) if data else "Kh√¥ng c√≥ d·ªØ li·ªáu")
    
    # Th·ª≠ l·∫•y gi√° t·ª´ c√°c c·ªôt c√≥ th·ªÉ ch·ª©a gi√°
    price_dict = {}
    for row in data:
        name_key = row.get("T√™n SP ƒë√£ s·ª≠a") or row.get("T√™n s·∫£n ph·∫©m")
        if not name_key:
            continue
        name_key = name_key.lower().strip()
        price = None
        if "Gi√° b√°n VC" in row:
            price = row["Gi√° b√°n VC"]
        elif "Gi√° b√°n (VNƒê)" in row:
            price = row["Gi√° b√°n (VNƒê)"]
        elif "Gi√°" in row:
            price = row["Gi√°"]
        # L∆∞u c·∫£ chu·ªói g·ªëc l√†m key
        price_dict[name_key] = price

        # Split theo / n·∫øu t·∫•t c·∫£ c√°c ph·∫ßn ƒë·ªÅu c√≥ factor
        parts = [p.strip() for p in name_key.split('/')]
        if len(parts) > 1 and all(has_factor(p) for p in parts):
            for part in parts:
                price_dict[part] = price
    
    return price_dict

def add_5giay_price_and_diff(df):
    # ƒê·ªïi t√™n c√°c c·ªôt tr∆∞·ªõc
    df = df.rename(columns={
        "T√™n s·∫£n ph·∫©m": "T√™n SP MKCOM",
        "Gi√° b√°n (VNƒê)": "Gi√° MKCOM"
    })

    prices_5giay = []
    diffs = []
    all_5giay_prices = get_all_5giay_prices(SHEET_5GIAY_URL, today_str)
    
    # In ra s·ªë l∆∞·ª£ng s·∫£n ph·∫©m t√¨m th·∫•y gi√°
    found_prices = sum(1 for price in all_5giay_prices.values() if price is not None)
    print(f"T√¨m th·∫•y gi√° cho {found_prices}/{len(all_5giay_prices)} s·∫£n ph·∫©m")
    
    for idx, row in df.iterrows():
        product_name = row["T√™n SP MKCOM"].lower().strip()
        price_5giay = get_price_from_5giay(product_name, all_5giay_prices)
        prices_5giay.append(price_5giay)
        
        # T√≠nh ch√™nh l·ªách ch·ªâ khi c√≥ gi√° ·ªü c·∫£ 2 b√™n
        if price_5giay != "-" and row["Gi√° MKCOM"] is not None:
            try:
                diff = row["Gi√° MKCOM"] - int(price_5giay)
                # N·∫øu ch√™nh l·ªách = 0 th√¨ g√°n "-"
                if diff == 0:
                    diff = "-"
            except:
                diff = ""
        else:
            diff = ""
        diffs.append(diff)
    
    # X√≥a c·ªôt Trang
    df = df.drop(columns=["Trang"])
    
    # Th√™m c·ªôt Gi√° 5giay v√† Ch√™nh l·ªách
    cols = list(df.columns)
    price_col_idx = cols.index("Gi√° MKCOM")
    df.insert(price_col_idx + 1, "Gi√° 5giay", prices_5giay)
    df.insert(price_col_idx + 2, "Ch√™nh l·ªách", diffs)
    
    # Th√™m link v√† c√°c c·ªôt kh√°c
    df["Link"] = df.apply(lambda row: make_text_fragment_link(row["Link"], row["T√™n SP MKCOM"]), axis=1)
    
    cols = df.columns.tolist()
    cols = ['T√™n Post'] + [col for col in cols if col != 'T√™n Post']
    df = df[cols]
    
    return df

def chuan_hoa_ten(name):
    return re.sub(r'\s*([\\/|])\s*', r'\1', name.strip().lower())

def extract_model_part(name):
    # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ d·ªÖ x·ª≠ l√Ω
    name = name.lower()
    # Lo·∫°i b·ªè ti·ªÅn t·ªë "barebone" v√† h√£ng n·∫øu c√≥
    # V√≠ d·ª•: "barebone dell 400g1 sff" -> "400g1 sff"
    match = re.search(r'barebone\s+(dell|hp|lenovo)?\s*(.*)', name)
    if match:
        return match.group(2).strip()
    return name.strip()

def get_price_from_5giay(name, price_dict):
    name = name.lower().strip()
    # So kh·ªõp tr·ª±c ti·∫øp v·ªõi t√™n SP ƒë√£ s·ª≠a
    if name in price_dict:
        return price_dict[name]
    return "-"  # Tr·∫£ v·ªÅ "-" n·∫øu kh√¥ng t√¨m th·∫•y t√™n gi·ªëng nhau

def make_text_fragment_link(base_url, product_name):
    encoded_text = urllib.parse.quote(product_name)
    return f"{base_url}#:~:text={encoded_text}"

def add_arrow_to_price(row):
    diff = row["Ch√™nh l·ªách"]
    price = row["Gi√° MKCOM"]
    if diff == "" or price == "":
        return price
    try:
        diff_val = int(diff)
        if diff_val == 0:
            return f"{price:,}"
        elif diff_val < 0:
            arrow = "üî∫"  # M≈©i t√™n l√™n
        else:
            arrow = "üîª"  # M≈©i t√™n xu·ªëng
        return f"{arrow} {price:,}"
    except:
        return price

def clear_duplicate_post_title(df):
    df = df.copy()
    df['T√™n Post'] = df['T√™n Post'].where(df['T√™n Post'].ne(df['T√™n Post'].shift()))
    return df

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def get_post_id_from_shortlink(product_url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        resp = requests.get(product_url, headers=headers, timeout=5)  # Gi·∫£m timeout xu·ªëng 5s
        if resp.status_code != 200:
            return ""
        soup = BeautifulSoup(resp.text, "html.parser")
        shortlink_tag = soup.find("link", rel="shortlink")
        if shortlink_tag and "href" in shortlink_tag.attrs:
            match = re.search(r"p=(\d+)", shortlink_tag["href"])
            if match:
                return match.group(1)
        return ""
    except Exception as e:
        print(f"L·ªói khi l·∫•y post ID t·ª´ {product_url}: {str(e)}")
        return ""

def add_post_id_column(df):
    # Gi·∫£ s·ª≠ c·ªôt "Link" ch·ª©a link s·∫£n ph·∫©m
    ids = []
    for url in df["Link"]:
        post_id = get_post_id_from_shortlink(url)
        ids.append(post_id)
    # Th√™m c·ªôt "ID" sau c·ªôt "Ch√™nh l·ªách"
    insert_idx = df.columns.get_loc("Ch√™nh l·ªách") + 1
    df.insert(insert_idx, "ID", ids)
    return df

def add_edit_price_column(df):
    # L·∫•y ID t·ª´ c·ªôt "ID" v√† t·∫°o link s·ª≠a (KH√îNG th√™m text fragment)
    edit_links = []
    for post_id in df["ID"]:
        if post_id:
            edit_link = f"https://minhkhoicomputer.com/wp-admin/post.php?post={post_id}&action=edit"
        else:
            edit_link = ""
        edit_links.append(edit_link)
    # Th√™m c·ªôt "S·ª≠a Gi√°" sau c·ªôt "Link"
    insert_idx = df.columns.get_loc("Link") + 1
    df.insert(insert_idx, "S·ª≠a Gi√°", edit_links)
    # X√≥a c·ªôt "ID"
    df = df.drop(columns=["ID"])
    return df

if __name__ == "__main__":
    all_links = get_all_barebone_links()
    print(f"T·ªïng s·ªë link s·∫£n ph·∫©m: {len(all_links)}")
    all_products = []
    for idx, (link, page) in enumerate(all_links, 1):
        print(f"({idx}/{len(all_links)}) ƒêang l·∫•y: {link} (Trang {page})")
        infos = get_barebone_info(link, page)
        all_products.extend(infos)
        time.sleep(1)
    # Chuy·ªÉn sang DataFrame
    df = pd.DataFrame(all_products)
    # Upload l√™n Google Sheets
    df = add_5giay_price_and_diff(df)
    df["Gi√° MKCOM"] = df.apply(add_arrow_to_price, axis=1)
    df = clear_duplicate_post_title(df)
    df = df.loc[df['T√™n Post'].ne(df['T√™n Post'].shift())].reset_index(drop=True)
    total = df['T√™n Post'].replace('', pd.NA).dropna().shape[0]
    df = df.rename(columns={"T√™n Post": f"T√™n Post [{total}]"})
    # Th√™m post ID v√† link s·ª≠a
    df = add_post_id_column(df)
    df = add_edit_price_column(df)
    upload_to_gsheets(df, SHEET_URL, worksheet_name=compare_sheet_name)
